{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f956f944-6fa1-4c8c-a93c-55d48307fb41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Fabric Notebook 2: Clustering (KMeans with automatic k)\n",
    "# ======================================\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup MLflow\n",
    "mlflow.set_experiment(\"Retail_ML_Experiments\")\n",
    "client = MlflowClient()\n",
    "registry_name = \"retail_ML_clustering\"\n",
    "try:\n",
    "    client.create_registered_model(registry_name)\n",
    "except:\n",
    "    print(f\"ℹ️ Registry {registry_name} already exists\")\n",
    "\n",
    "# Load data\n",
    "df = spark.table(\"Gold_Customer_LTV\")\n",
    "\n",
    "# Assemble numeric features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Lifetime_Spend\", \"Num_Transactions\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Find the best k\n",
    "# -------------------------------\n",
    "wssse_list = []\n",
    "k_values = list(range(2, 11))  # test k from 2 to 10\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(k=k, seed=42, featuresCol=\"features\")\n",
    "    model = kmeans.fit(data)\n",
    "    wssse = model.summary.trainingCost  # Sum of squared distances to cluster centers\n",
    "    wssse_list.append(wssse)\n",
    "    print(f\"k={k}, WSSSE={wssse}\")\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_values, wssse_list, marker='o')\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"WSSSE\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose best k based on the elbow method (for demo, pick the k with largest drop)\n",
    "best_k = k_values[wssse_list.index(min(wssse_list))]  # can be refined manually after inspecting the plot\n",
    "print(f\"✅ Selected k={best_k}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Train final model with best k\n",
    "# -------------------------------\n",
    "with mlflow.start_run(run_name=f\"KMeans_k{best_k}\") as run:\n",
    "    kmeans = KMeans(\n",
    "        k=best_k,\n",
    "        seed=42,\n",
    "        featuresCol=\"features\",\n",
    "        predictionCol=\"cluster\"\n",
    "    )\n",
    "    model = kmeans.fit(data)\n",
    "    clusters = model.transform(data)\n",
    "\n",
    "    # Log parameters and model\n",
    "    mlflow.log_param(\"k\", best_k)\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "    # Register model\n",
    "    try:\n",
    "        mv = client.create_model_version(\n",
    "            name=registry_name,\n",
    "            source=f\"runs:/{run.info.run_id}/model\",\n",
    "            run_id=run.info.run_id\n",
    "        )\n",
    "        print(f\"Model registered as version {mv.version}\")\n",
    "    except:\n",
    "        print(\"Registry skipped\")\n",
    "\n",
    "# Save clustering results for Power BI\n",
    "clusters.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"PowerBI_Customer_Segments\")\n",
    "print(\"✅ Saved clustering results to PowerBI_Customer_Segments\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Optional 2D visualization\n",
    "# -------------------------------\n",
    "# Collect data to driver for plotting (only feasible for small datasets)\n",
    "plot_df = clusters.select(\"Lifetime_Spend\", \"Num_Transactions\", \"cluster\").toPandas()\n",
    "plt.figure(figsize=(8,6))\n",
    "for c in plot_df['cluster'].unique():\n",
    "    subset = plot_df[plot_df['cluster']==c]\n",
    "    plt.scatter(subset['Lifetime_Spend'], subset['Num_Transactions'], label=f'Cluster {c}')\n",
    "plt.xlabel(\"Lifetime Spend\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.title(\"Customer Segments\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Retail_ml_Cluster",
   "widgets": {}
  },
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
