{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab30ac43-acee-4af8-82c6-bc22e60e665c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Databricks Free Edition - KMeans Clustering (sklearn)\n",
    "# ======================================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import mlflow\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================================\n",
    "# MLflow setup\n",
    "# ======================================\n",
    "mlflow.set_experiment(\"/Users/amirrezakha@yahoo.com/Retail_ML_Experiments\")\n",
    "mlflow.login()\n",
    "experiment_name = \"/Users/amirrezakha@yahoo.com/Retail_ML_Experiments\"\n",
    "\n",
    "# ======================================\n",
    "# Load data\n",
    "# ======================================\n",
    "df_spark = spark.table(\"Gold_Customer_LTV\")\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "# Features for clustering\n",
    "feature_cols = [\"Lifetime_Spend\", \"Num_Transactions\"]\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ======================================\n",
    "# Step 1: Find best k using Elbow method\n",
    "# ======================================\n",
    "wssse_list = []\n",
    "k_values = list(range(2, 11))\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_scaled)\n",
    "    wssse = km.inertia_  # Sum of squared distances\n",
    "    wssse_list.append(wssse)\n",
    "    print(f\"k={k}, WSSSE={wssse}\")\n",
    "\n",
    "# Plot elbow\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_values, wssse_list, marker='o')\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"WSSSE\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Select best k (example: pick k with largest drop)\n",
    "best_k = k_values[wssse_list.index(min(wssse_list))]\n",
    "print(f\"✅ Selected k={best_k}\")\n",
    "\n",
    "# ======================================\n",
    "# Step 2: Train final KMeans model\n",
    "# ======================================\n",
    "with mlflow.start_run(run_name=f\"sklearn_KMeans_k{best_k}\") as run:\n",
    "    km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    km_final.fit(X_scaled)\n",
    "    clusters = km_final.labels_\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"k\", best_k)\n",
    "    mlflow.log_param(\"features\", feature_cols)\n",
    "    \n",
    "    # Save model locally and log artifact\n",
    "    local_model_path = f\"/tmp/sklearn_kmeans.pkl\"\n",
    "    joblib.dump(km_final, local_model_path)\n",
    "    mlflow.log_artifact(local_model_path, artifact_path=\"model\")\n",
    "    \n",
    "    # Attach clusters to dataframe\n",
    "    df[\"cluster\"] = clusters\n",
    "\n",
    "# ======================================\n",
    "# Step 3: Save clustering results for Power BI\n",
    "# ======================================\n",
    "# Convert back to Spark DataFrame\n",
    "df_spark_out = spark.createDataFrame(df)\n",
    "df_spark_out.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"PowerBI_Customer_Segments\")\n",
    "print(\"✅ Saved clustering results to PowerBI_Customer_Segments\")\n",
    "\n",
    "# ======================================\n",
    "# Step 4: 2D scatter plot\n",
    "# ======================================\n",
    "plt.figure(figsize=(8,6))\n",
    "colors = ['red','green','blue','orange','purple','brown']\n",
    "for c in range(best_k):\n",
    "    subset = df[df['cluster']==c]\n",
    "    plt.scatter(subset['Lifetime_Spend'], subset['Num_Transactions'], \n",
    "                label=f'Cluster {c}', color=colors[c % len(colors)])\n",
    "plt.xlabel(\"Lifetime Spend\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.title(\"Customer Segments\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c5369a-cf5d-4112-8315-77ee3f6e2915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Databricks Free Edition - KMeans / GMM clustering\n",
    "# ======================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import mlflow\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLflow workaround\n",
    "import databricks.connect as db_connect\n",
    "import mlflow.tracking._model_registry.utils\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "mlflow.login()\n",
    "\n",
    "# Load data\n",
    "df = spark.table(\"Gold_Customer_LTV\").toPandas()\n",
    "features = [\"Lifetime_Spend\", \"Num_Transactions\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[features])\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_experiment(\"/Users/amirrezakha@yahoo.com/Retail_ML_Experiments\")\n",
    "\n",
    "# ----------------------\n",
    "# KMeans\n",
    "# ----------------------\n",
    "with mlflow.start_run(run_name=\"KMeans\") as run:\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    df['cluster_kmeans'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    mlflow.log_param(\"algorithm\", \"KMeans\")\n",
    "    mlflow.log_param(\"n_clusters\", 3)\n",
    "    \n",
    "    local_path = \"/tmp/kmeans_clusters.csv\"\n",
    "    df.to_csv(local_path, index=False)\n",
    "    mlflow.log_artifact(local_path, artifact_path=\"clusters\")\n",
    "    \n",
    "    print(\"KMeans unique clusters:\", df['cluster_kmeans'].unique())\n",
    "\n",
    "# ----------------------\n",
    "# GMM\n",
    "# ----------------------\n",
    "with mlflow.start_run(run_name=\"GMM\") as run:\n",
    "    gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "    df['cluster_gmm'] = gmm.fit_predict(X_scaled)\n",
    "    \n",
    "    mlflow.log_param(\"algorithm\", \"GMM\")\n",
    "    mlflow.log_param(\"n_components\", 3)\n",
    "    \n",
    "    local_path = \"/tmp/gmm_clusters.csv\"\n",
    "    df.to_csv(local_path, index=False)\n",
    "    mlflow.log_artifact(local_path, artifact_path=\"clusters\")\n",
    "    \n",
    "    print(\"GMM unique clusters:\", df['cluster_gmm'].unique())\n",
    "\n",
    "# ----------------------\n",
    "# Visualization (sample 50% to save memory)\n",
    "# ----------------------\n",
    "plot_df = df.sample(frac=0.5, random_state=42)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(plot_df['Lifetime_Spend'], plot_df['Num_Transactions'], c=plot_df['cluster_kmeans'], cmap='viridis')\n",
    "plt.title(\"KMeans Clusters\")\n",
    "plt.xlabel(\"Lifetime Spend\")\n",
    "plt.ylabel(\"Num Transactions\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(plot_df['Lifetime_Spend'], plot_df['Num_Transactions'], c=plot_df['cluster_gmm'], cmap='plasma')\n",
    "plt.title(\"GMM Clusters\")\n",
    "plt.xlabel(\"Lifetime Spend\")\n",
    "plt.ylabel(\"Num Transactions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail_ml_Cluster",
   "widgets": {}
  },
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
