{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15c1cd6-2aae-47d9-8748-ddc0ae901978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from retail_transactions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba4f9ed-0cf4-4f6e-8437-4d582d959800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Databricks Free Edition - FP-Growth (Fixed Conversion + Rules)\n",
    "# ======================================\n",
    "\n",
    "# --- Install required library ---\n",
    "%pip install mlxtend --quiet\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "import mlflow\n",
    "import databricks.connect as db_connect\n",
    "import mlflow.tracking._model_registry.utils\n",
    "\n",
    "# ======================================\n",
    "# MLflow setup\n",
    "# ======================================\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "mlflow.login()  # INFO-log: Login successful!\n",
    "\n",
    "mlflow.set_experiment(\"/Users/amirrezakha@yahoo.com/Retail_ML_Experiments\")\n",
    "\n",
    "# ======================================\n",
    "# Load and prepare data\n",
    "# ======================================\n",
    "df_spark = spark.table(\"retail_transactions_dataset\").select(\"Transaction_ID\", \"Product\")\n",
    "df_sample = df_spark.limit(10000)\n",
    "df = df_sample.toPandas()\n",
    "print(f\"‚úÖ Loaded {len(df)} rows from retail_transactions_dataset\")\n",
    "\n",
    "# Validate\n",
    "if not {\"Transaction_ID\", \"Product\"}.issubset(df.columns):\n",
    "    raise ValueError(\"Table must contain columns: Transaction_ID, Product\")\n",
    "\n",
    "print(f\"üßæ Unique transactions: {df['Transaction_ID'].nunique()}\")\n",
    "print(f\"üõçÔ∏è Unique products: {df['Product'].nunique()}\")\n",
    "\n",
    "# One-hot encode\n",
    "basket = df.groupby(['Transaction_ID', 'Product']).size().unstack(fill_value=0)\n",
    "basket = (basket > 0).astype(int)\n",
    "print(f\"‚úÖ Basket matrix shape: {basket.shape}\")\n",
    "\n",
    "# ======================================\n",
    "# Train FP-Growth & Association Rules\n",
    "# ======================================\n",
    "with mlflow.start_run(run_name=\"FPGrowth_mlxtend\") as run:\n",
    "    frequent_itemsets = fpgrowth(basket, min_support=0.001, use_colnames=True)\n",
    "    frequent_itemsets[\"length\"] = frequent_itemsets[\"itemsets\"].apply(len)\n",
    "\n",
    "    print(f\"‚úÖ Frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "\n",
    "    # Try generating rules\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05)\n",
    "    print(f\"üìà Association rules found: {len(rules)}\")\n",
    "\n",
    "    # Convert sets to strings for Spark\n",
    "    frequent_itemsets[\"itemsets\"] = frequent_itemsets[\"itemsets\"].apply(lambda x: \",\".join(list(x)))\n",
    "    if not rules.empty:\n",
    "        for col in [\"antecedents\", \"consequents\"]:\n",
    "            rules[col] = rules[col].apply(lambda x: \",\".join(list(x)))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No association rules found. Try reducing confidence threshold or expanding dataset.\")\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_param(\"algorithm\", \"FPGrowth (mlxtend)\")\n",
    "    mlflow.log_param(\"min_support\", 0.001)\n",
    "    mlflow.log_param(\"min_confidence\", 0.05)\n",
    "    mlflow.log_metric(\"num_itemsets\", len(frequent_itemsets))\n",
    "    mlflow.log_metric(\"num_rules\", len(rules))\n",
    "\n",
    "    # Save results locally\n",
    "    fi_path = \"/tmp/frequent_itemsets.csv\"\n",
    "    rules_path = \"/tmp/association_rules.csv\"\n",
    "    frequent_itemsets.to_csv(fi_path, index=False)\n",
    "    rules.to_csv(rules_path, index=False)\n",
    "    mlflow.log_artifact(fi_path, artifact_path=\"output\")\n",
    "    mlflow.log_artifact(rules_path, artifact_path=\"output\")\n",
    "\n",
    "    # ======================================\n",
    "    # Convert results to Spark for Power BI\n",
    "    # ======================================\n",
    "    fi_spark = spark.createDataFrame(frequent_itemsets)\n",
    "    fi_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"PowerBI_Frequent_Itemsets\")\n",
    "\n",
    "    if not rules.empty:\n",
    "        rules_spark = spark.createDataFrame(rules)\n",
    "        rules_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"PowerBI_Basket_Analysis\")\n",
    "        print(\"‚úÖ Saved rules to PowerBI_Basket_Analysis\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipped saving rules (empty DataFrame).\")\n",
    "\n",
    "    print(\"‚úÖ Saved frequent itemsets to PowerBI_Frequent_Itemsets\")\n",
    "\n",
    "# ======================================\n",
    "# Display preview\n",
    "# ======================================\n",
    "print(\"üìä Frequent Itemsets:\")\n",
    "display(fi_spark.limit(10))\n",
    "\n",
    "if not rules.empty:\n",
    "    print(\"üìà Association Rules:\")\n",
    "    display(rules_spark.limit(10))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5430116362362844,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail_ml_Associate",
   "widgets": {}
  },
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
